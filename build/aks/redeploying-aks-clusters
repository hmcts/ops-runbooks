<h1 id="redeploying-aks-clusters-runbook">Redeploying AKS Clusters Runbook</h1>
<p>This wiki page documents some tasks that we have to perform when deploying any of the AKS Clusters</p>
<h2 id="to-create-a-cluster">To create a cluster:</h2>
<p>Go to the <a href="https://dev.azure.com/hmcts/CNP/_build?definitionId=483&amp;_a=summary">pipeline</a>.</p>
<p>Click on <strong>Run pipeline</strong> (blue button) in top right of Azure DevOps. Ensure that Action is set to Apply, Cluster is set to which ever one you want to build and that the environment is selected from the drop down menu.</p>
<p>Click on <strong>Run</strong> to start the pipeline.</p>
<h2 id="deployment-order">Deployment Order</h2>
<p>This order can change; created to give a brief overview of the environments.</p>
<p>Redeploying in order:-</p>

<ul>
<li>sbox             - (K8s service names are  = cft-sbox-00-aks - cft-sbox-01-aks)</li>
<li>Management sbox  - (K8s service name is    = cft-ptlsbox-00-aks)</li>
<li>ITHC             - (K8s service names are  = cft-ithc-00-aks - cft-ithcx-01-aks)</li>
<li>Preview          - (K8s service name is    = cft-previewx-01-aks)</li>
<li>AAT              - (K8s service names are  = cft-aat-00-aks - cft-aat-01-aks)</li>
<li>Production       - (K8s service names are  = prod-00-aks - prod-01-aks) </li>
<li>Management       - (K8s service name is    = cft-ptl-00-aks) </li>
<li>Perftest         - (K8s service names are  = cft-perftest-00-aks - cft-perftest-01-aks)</li>
<li>Demo             - (K8s service names are  = cft-demo-00-aks - cft-demo-01-aks)</li>
</ul>
<h2 id="dashboards">Dashboards</h2>
<p>A good indication along with reviewing the environments themselves, reviewing Grafana
<a href="grafana.sandbox.platform.hmcts.net">Grafana Sandbox</a>
<a href="https://grafana-ptl.platform.hmcts.net/">Grafana</a></p>
<h2 id="environment-specifics">Environment specifics</h2>
<p>Some environments do have some additional requirements/checks that need to confirmed prior to deploying any of its clusters.</p>
<h3 id="sandbox">Sandbox</h3>
<p>N/A</p>
<h3 id="management-sandbox">Management Sandbox</h3>
<h4 id="before-deployment-of-a-cluster">Before deployment of a cluster</h4>
<p>As this environment only tends to be used by IDAM, you need to confirm with Paul Verity that the Sandbox Jenkins instance which sits on this cluster isn&rsquo;t being used and that he is ok with the work to go ahead. The sandbox Jenkins instance can be logged into <a href="https://sandbox-build.platform.hmcts.net/">here</a>.</p>
<p>Confirm that the <a href="https://sandbox-build.platform.hmcts.net/job/HMCTS_Sandbox_CNP/job/cnp-plum-recipes-service/job/master/">cnp-plum-recipes-service</a> job within Jenkins runs successfully.</p>

<ul>
<li><p>Stop the cluster and once the cluster has fully stopped take a snapshot of the disk referenced <a href="https://github.com/hmcts/cnp-flux-config/blob/85d61449e8633c6a975798c01e7ce155c9861c7e/apps/jenkins/jenkins/sbox-intsvc/disk.yaml#L8">here</a> and ensure it is placed somewhere safe so that we can re-use this snapshot to re-create the jenkins disk after the cluster has been redeployed.</p></li>
<li><p>Run this <a href="https://dev.azure.com/hmcts/CNP/_build?definitionId=483&amp;_a=summary">Pipeline</a> to destroy the existing cluster, the environment is <strong>PTLSBOX</strong>.</p></li>
<li><p>Run the same <a href="https://dev.azure.com/hmcts/CNP/_build?definitionId=483&amp;_a=summary">Pipeline</a> again to apply to the cluster, the environment is <strong>PTLSBOX</strong>.</p></li>
<li><p>Once the pipeline to deploy the cluster has completed, refer to the snapshot previously taken and create a new disk from it named as <strong>jenkins-disk</strong> ensuring it is placed in the location referenced <a href="https://github.com/hmcts/cnp-flux-config/blob/85d61449e8633c6a975798c01e7ce155c9861c7e/apps/jenkins/jenkins/sbox-intsvc/disk.yaml#L8">here</a>.</p></li>
</ul>
<h4 id="after-deployment-of-a-cluster">After deployment of a cluster</h4>
<p>Once the cluster has been redeployed and Jenkins is back up and running, test the <a href="https://sandbox-build.platform.hmcts.net/job/HMCTS_Sandbox_CNP/job/cnp-plum-recipes-service/job/master/">cnp-plum-recipes-service</a> job again to confirm it runs successfully as it did before (it should).</p>
<h3 id="ithc">ITHC</h3>
<h4 id="ithc-before-deployment-of-a-cluster">Before deployment of a cluster</h4>

<ul>
<li>Remove the cluster you are going to redeploying from the AGW. <a href="https://github.com/hmcts/azure-platform-terraform/pull/546">PR example here</a></li>
<li>Unsure which IP belongs to which cluster? Check the front end IP of the kubernetes-internal loadbalancer</li>
</ul>
<h4 id="ithc-after-deployment-of-a-cluster">After deployment of a cluster</h4>

<ul>
<li>Add the cluster back into AGW once you have confirmed deployment has been successful. <a href="https://github.com/hmcts/azure-platform-terraform/pull/560">PR example here</a></li>
</ul>
<h3 id="preview">Preview</h3>
<p>We have the ability to create another preview cluster on demand. We don&rsquo;t run with two at a time as they&rsquo;re 75 node clusters and that would be quite expensive.</p>
<h4 id="when-the-build-has-finished">When the build has finished</h4>

<ul>
<li>Check the system helm releases and pods are up: flux, flux-helm-operator, tunnelfront, coredns, nodelocaldns, - aad-pod-identity, traefik</li>
<li>Check an external IP has been assigned: kubectl get service -n admin |awk &lsquo;$4 ~ /^[0-9]/&rsquo;</li>
<li>Check OSBA is running, kubectl get pods -n osba, kubectl get pods -n catalog</li>
</ul>
<h4 id="how-to-test-a-specific-preview-cluster-without-swapping-over">How to test a specific Preview cluster without swapping over</h4>
<p>https://github.com/hmcts/cnp-plum-recipes-service/pull/379/files</p>
<p>https://github.com/hmcts/cnp-jenkins-library/compare/preview01?expand=1</p>
<p>This will simulate a repo/setup is using the Preview cluster that has not been swapped, useful to test if required.
Same PR might not be re-usable when the cluster is swapped to active as DNS record could be invalid. You can verify and manually tweak the records to be able to reuse it.</p>
<h4 id="preview-after-deployment-of-a-cluster">After deployment of a cluster</h4>

<ul>
<li>Change Jenkins to use the other cluster, e.g. <a href="https://github.com/hmcts/cnp-flux-config/pull/19886/files">cnp-flux-config#19886</a> .

<ul>
<li>If not updated, you can change manually providing PR has been approved and merged <a href="https://build.platform.hmcts.net/configure">View Jenkins Configuration</a></li>
</ul></li>
<li>Swap external DNS active cluster, e.g. <a href="https://github.com/hmcts/cnp-flux-config/pull/19889">cnp-flux-config#19889</a> </li>
<li>Create a test PR, normally we just make a README change to <a href="https://github.com/hmcts/rpe-pdf-service">rpe-pdf-service</a> . <a href="https://github.com/hmcts/rpe-pdf-service/pull/318">example PR</a>

<ul>
<li>Check this PR in Jenkins has successfully ran through stage <a href="https://build.platform.hmcts.net/job/HMCTS_Platform/job/rpe-pdf-service/view/change-requests/job/PR-318/">AKS deploy - preview</a></li>
</ul></li>
<li>Update the charts for jobs and steps pointing to the new preview <a href="https://github.com/hmcts/cnp-azuredevops-libraries/pull/127/files">cnp-azuredevops-pipelines#127</a></li>
<li>Send across the comms on #cloud-native-announce channel regarding the swap over to the new preview cluster, see below example annoucenment:-</li>
</ul>
<p>Hi all, Preview cluster has been swapped cft-preview-00-aks.
Login using:
<code>command
az aks get-credentials --resource-group cft-preview-00-rg --name cft-preview-00-aks --subscription DCD-CFTAPPS-DEV --overwrite
</code></p>

<ul>
<li>Delete all ingress on the old cluster to ensure external-dns deletes it&rsquo;s existing records:</li>
</ul>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>kubectl delete ingress --all-namespaces -l app.kubernetes.io/managed-by=Helm
</code></pre></div>
<ul>
<li>Delete any orphan A records that external-dns might have missed:</li>
</ul>
<p><em>Replace 10.12.79.250 with the loadbalancer IP (kubernetes-internal) of the cluster you want to cleanup</em>
&ldquo;`command</p>
<h1 id="private-dns">Private DNS</h1>
<p>az network private-dns record-set a list &ndash;zone-name service.core-compute-preview.internal -g core-infra-intsvc-rg &ndash;subscription DTS-CFTPTL-INTSVC &ndash;query &rdquo;[?aRecords[0].ipv4Address==&lsquo;10.12.79.250&rsquo;].[name]&ldquo; -o tsv | xargs -I {} -n 1 -P 8 az network private-dns record-set a delete &ndash;zone-name service.core-compute-preview.internal -g core-infra-intsvc-rg &ndash;subscription DTS-CFTPTL-INTSVC &ndash;yes &ndash;name {}</p>
<h1 id="public-dns">Public DNS</h1>
<p>az network dns record-set a list &ndash;zone-name demo.platform.hmcts.net -g reformmgmtrg &ndash;subscription Reform-CFT-Mgmt &ndash;query &rdquo;[?aRecords[0].ipv4Address==&lsquo;20.90.254.226&rsquo;].[name]&ldquo; -o tsv | xargs -I {} -n 1 -P 8 az network dns record-set a delete &ndash;zone-name demo.platform.hmcts.net -g reformmgmtrg &ndash;subscription Reform-CFT-Mgmt &ndash;yes &ndash;name {}
&rdquo;`</p>
<p>Deletes any txt records pointing to inactive:</p>
<div class="highlight"><pre class="highlight shell" tabindex="0"><code><span class="c"># Private DNS</span>
az network private-dns record-set txt list <span class="nt">--zone-name</span> demo.platform.hmcts.net <span class="nt">-g</span> core-infra-intsvc-rg <span class="nt">--subscription</span> DTS-CFTPTL-INTSVC <span class="nt">--query</span> <span class="s2">"[?contains(txtRecords[0].value[0], 'inactive')].[name]"</span> <span class="nt">-o</span> tsv | xargs <span class="nt">-I</span> <span class="o">{}</span> <span class="nt">-n</span> 1 <span class="nt">-P</span> 8 az network private-dns record-set txt delete <span class="nt">--zone-name</span> demo.platform.hmcts.net <span class="nt">-g</span> core-infra-intsvc-rg <span class="nt">--subscription</span> DTS-CFTPTL-INTSVC <span class="nt">--yes</span> <span class="nt">--name</span> <span class="o">{}</span>
<span class="c"># Public DNS</span>
az network dns record-set txt list <span class="nt">--zone-name</span> demo.platform.hmcts.net <span class="nt">-g</span> reformmgmtrg <span class="nt">--subscription</span> Reform-CFT-Mgmt <span class="nt">--query</span> <span class="s2">"[?contains(txtRecords[0].value[0], 'inactive')].[name]"</span> <span class="nt">-o</span> tsv | xargs <span class="nt">-I</span> <span class="o">{}</span> <span class="nt">-n</span> 1 <span class="nt">-P</span> 8 az network dns record-set txt delete <span class="nt">--zone-name</span> demo.platform.hmcts.net <span class="nt">-g</span> reformmgmtrg <span class="nt">--subscription</span> Reform-CFT-Mgmt <span class="nt">--yes</span> <span class="nt">--name</span> <span class="o">{}</span>
</code></pre></div><p>Once swap over is fully complete then delete the older cluster,</p>

<ul>
<li>Run the <a href="https://dev.azure.com/hmcts/CNP/_build?definitionId=483&amp;_a=summary">Pipeline</a> ensuring Action is set to Destroy, Cluster is set to cluster you plan to destroy and Environment is set to that you intend to run against before clicking on <strong>Run</strong>. 
### AAT</li>
</ul>
<h4 id="public-dns-before-deployment-of-a-cluster">Before deployment of a cluster</h4>

<ul>
<li>Remove the cluster you are going to redeploying from the AGW. <a href="https://github.com/hmcts/azure-platform-terraform/pull/580">PR example here</a></li>
<li>Unsure which IP belongs to which cluster? Check the front end IP of the kubernetes-internal loadbalancer</li>
<li>Change Jenkins to use the other cluster that is not going to be redeployed, e.g. <a href="https://github.com/hmcts/cnp-flux-config/pull/7138/files">PR example here</a> .

<ul>
<li>If not updated, you can change manually providing PR has been approved and merged <a href="https://build.platform.hmcts.net/configure">View Jenkins Configuration</a></li>
</ul></li>
</ul>
<h4 id="public-dns-after-deployment-of-a-cluster">After deployment of a cluster</h4>

<ul>
<li>Add the cluster back into AGW once you have confirmed deployment has been successful. <a href="https://github.com/hmcts/azure-platform-terraform/pull/582">PR example here</a></li>
<li>Update Civil team&rsquo;s DNS entries to point to the active Jenkins cluster (until they switch to using platform-hmcts-net). <a href="https://github.com/hmcts/azure-private-dns/pull/428/files">Update active jenkins-cluster IP value in DNS</a> and <a href="https://github.com/hmcts/azure-private-dns/pull/430/files">update existing platform-hmcts-net entries</a>.</li>
</ul>
<h3 id="production">Production</h3>
<h4 id="day-or-hours-before-deployment-of-a-cluster">Day or hours before deployment of a cluster</h4>
<p>It is important to identify applications with underlying issues and allow sufficient time for teams to acknowledge or fix issues before proceeding with cluster rebuild.
- Create a time-snapshot list of pods for comparison reference post-deployment - <code>TIMESTAMP=&quot;$(date +%F_%H%M%S)&quot; &amp;&amp; kubectl get pods -A &gt; all_pods_status_$TIMESTAMP</code>
- Create a time-snapshot count of pods for comparison reference post-deployment - <code>TIMESTAMP=&quot;$(date +%F_%H%M%S)&quot; &amp;&amp; kubectl get pods -A | wc -l &gt; total_pods_$TIMESTAMP</code><br>
- To identity failed Helm releases and copy list into a file, run <code>TIMESTAMP=&quot;$(date +%F_%H%M%S)&quot; &amp;&amp; kubectl get hr -A | grep -v Succeeded &gt; failed_hrs_$TIMESTAMP</code><br>
- To identify failed pods and copy list into a file, run <code>TIMESTAMP=&quot;$(date +%F_%H%M%S)&quot; &amp;&amp; kubectl get pods -A | grep -v Completed | grep -v Running &gt; failed_pods_$TIMESTAMP</code>
- Investigate failed helm releases and missing pods as required<br>
- For failed helm releases where pods are not deployed, test if rolling back to a previous release <code>helm rollback</code> is possible (which helps narrow down issue being specific to current release) 
- For failed pods, investigate root cause and discuss with teams as required (e.g. pods not starting due to missing keyvault secrets)</p>
<h4 id="production-before-deployment-of-a-cluster">Before deployment of a cluster</h4>

<ul>
<li>Remove the cluster you are going to be redeploying from the AGW. <a href="https://github.com/hmcts/azure-platform-terraform/pull/594">PR example here</a></li>
<li>Unsure which IP belongs to which cluster? Check the front end IP of the kubernetes-internal loadbalancer</li>
</ul>
<p>Every Production change which involves taking down a cluster is supposed to include scaling up of the apps (these can chan e):-
- idam-api (scale from 4 to 8)
- ccd-data-store-api (scale from 15 to 30)
- ccd-definition-store-api (scale from 4 to 8)
- dm-store (scale from 7 to 14)</p>
<p><em>(idam-api requires 8 pods to be split up across 2 clusters and handle the load.  If we are bringing a cluster down then we need to ensure the other cluster has 8 pods for idam-api)</em></p>
<p>This scaling is done via a PR <a href="https://github.com/hmcts/cnp-flux-config/pull/7245">PR example here</a>
Scaling to happen just before a cluster has been removed from AGW. 
- Create PR to scale apps and merge. 
- Check cluster that won&rsquo;t be removed to confirm pods have scaled
- Merge PR to remove a cluster from AGW</p>
<h4 id="production-after-deployment-of-a-cluster">After deployment of a cluster</h4>

<ul>
<li>Check all pods are deployed and running. Compare with pods status reference taken pre-deployment<br></li>
<li>Speak to teams (where required) for any specific issues related to failed pods</li>
<li>Ensure failed pods issues are either acknowledged by teams or fixed before rebuilding 2nd cluster.  This is to prevent a situation whereby applications are failed across both clusters after rebuild</li>
<li>Add the cluster back into AGW once you have confirmed deployment has been successful. <a href="https://github.com/hmcts/azure-platform-terraform/pull/595">PR example here</a></li>
<li>Revert merge for scaling pods &amp; merge <a href="https://github.com/hmcts/cnp-flux-config/pull/7245">PR example here</a></li>
<li>Confirm pods are back to correct numbers after revert</li>
</ul>
<h3 id="management-cftptl-intsvc">Management (cftptl-intsvc)</h3>
<p>Anything configuration wise i.e jobs, secrets, and config is stored externally from the Jenkins server, no requirement to back up the server.
See https://github.com/hmcts/cnp-flux-config/blob/master/apps/jenkins/jenkins/ptl-intsvc/jenkins.yaml</p>
<h4 id="management-cftptl-intsvc-before-deployment-of-a-cluster">Before deployment of a cluster</h4>

<ul>
<li>Make an announcement that Jenkins will be unavailable. This environment is best to be done early in the morning. Example announcement to send in the cloud-native-announce slack channel is:-</li>
</ul>

<blockquote>
<p>Hi @here,
Due to planned upgrades of AKS, we will be upgrading the management (cftplt-intsvc) cluster at 8am, Monday 26th April. As a result of this, Jenkins will be offline during the upgrade and &gt; unavailable for around one hour.&gt;</p>
</blockquote>

<ul>
<li><p>Run the <a href="https://dev.azure.com/hmcts/CNP/_build?definitionId=483&amp;_a=summary">Pipeline</a> ensuring Action is set to Destroy, Cluster is set to cluster you plan to destroy and Environment is set to <strong>PTL</strong> before clicking on <strong>Run</strong>.</p></li>
<li><p>Run the <a href="https://dev.azure.com/hmcts/CNP/_build?definitionId=483&amp;_a=summary">Pipeline</a> ensuring Action is set to Apply, Cluster is set to cluster you plan to build and Environment is set to <strong>PTL</strong> before clicking on <strong>Run</strong>.</p></li>
</ul>
<h4 id="management-cftptl-intsvc-after-deployment-of-a-cluster">After deployment of a cluster</h4>
<p>After the cluster has been redeployed successfully and hr&rsquo;s / pods are running as expected you need to verify that you can get to the <a href="https://build.platform.hmcts.net">Jenkins web ui</a>, and then comment on the slack channel announcement made previously to advise Jenkins is back up.</p>
<h3 id="perftest">Perftest</h3>
<h4 id="perftest-day-or-hours-before-deployment-of-a-cluster">Day or hours before deployment of a cluster</h4>
<p>It is important to identify applications with underlying issues and allow sufficient time for teams to acknowledge or fix issues before proceeding with cluster rebuild.
- Create a time-snapshot list of pods for comparison reference post-deployment - <code>TIMESTAMP=&quot;$(date +%F_%H%M%S)&quot; &amp;&amp; kubectl get pods -A &gt; all_pods_status_$TIMESTAMP</code>
- Create a time-snapshot count of pods for comparison reference post-deployment - <code>TIMESTAMP=&quot;$(date +%F_%H%M%S)&quot; &amp;&amp; kubectl get pods -A | wc -l &gt; total_pods_$TIMESTAMP</code><br>
- To identity failed Helm releases and copy list into a file, run <code>TIMESTAMP=&quot;$(date +%F_%H%M%S)&quot; &amp;&amp; kubectl get hr -A | grep -v Succeeded &gt; failed_hrs_$TIMESTAMP</code><br>
- To identify failed pods and copy list into a file, run <code>TIMESTAMP=&quot;$(date +%F_%H%M%S)&quot; &amp;&amp; kubectl get pods -A | grep -v Completed | grep -v Running &gt; failed_pods_$TIMESTAMP</code>
- Investigate failed helm releases and missing pods as required<br>
- For failed helm releases where pods are not deployed, test if rolling back to a previous release <code>helm rollback</code> is possible (which helps narrow down issue being specific to current release) 
- For failed pods, investigate root cause and discuss with teams as required (e.g. pods not starting due to missing keyvault secrets)
- For pods where images beginning with <em>pr-</em> aren&rsquo;t found (has been seen quite a few times on previous cluster rebuilds) this is often due to the image no longer existing in the ACR. In these instances you will need to either reach out to the app teams to get it updated or find the latest prod image for that pod in the ACR and put in a PR to fix like this one done previously - https://github.com/hmcts/cnp-flux-config/pull/17115/files.</p>
<h4 id="perftest-before-deployment-of-a-cluster">Before deployment of a cluster</h4>

<ul>
<li>Confirm that the environment isn&rsquo;t being used with Nickin Sitaram before starting. </li>
<li>Use slack channel pertest-cluster for communication.</li>
<li>Scale the number of active nodes, increase by 5 nodes if removing a custer. 
Reason for this CCD and IDAM will auto-scale the number of running pods when a cluster is taken out of service for a upgrade.</li>
</ul>
<h4 id="perftest-after-deployment-of-a-cluster">After deployment of a cluster</h4>

<ul>
<li>Check all pods are deployed and running. Compare with pods status reference taken pre-deployment
### Demo</li>
</ul>
<h4 id="perftest-day-or-hours-before-deployment-of-a-cluster-2">Day or hours before deployment of a cluster</h4>
<p>It is important to identify applications with underlying issues and allow sufficient time for teams to acknowledge or fix issues before proceeding with cluster rebuild.
- Create a time-snapshot list of pods for comparison reference post-deployment - <code>TIMESTAMP=&quot;$(date +%F_%H%M%S)&quot; &amp;&amp; kubectl get pods -A &gt; all_pods_status_$TIMESTAMP</code>
- Create a time-snapshot count of pods for comparison reference post-deployment - <code>TIMESTAMP=&quot;$(date +%F_%H%M%S)&quot; &amp;&amp; kubectl get pods -A | wc -l &gt; total_pods_$TIMESTAMP</code><br>
- To identity failed Helm releases and copy list into a file, run <code>TIMESTAMP=&quot;$(date +%F_%H%M%S)&quot; &amp;&amp; kubectl get hr -A | grep -v Succeeded &gt; failed_hrs_$TIMESTAMP</code><br>
- To identify failed pods and copy list into a file, run <code>TIMESTAMP=&quot;$(date +%F_%H%M%S)&quot; &amp;&amp; kubectl get pods -A | grep -v Completed | grep -v Running &gt; failed_pods_$TIMESTAMP</code>
- Investigate failed helm releases and missing pods as required<br>
- For failed helm releases where pods are not deployed, test if rolling back to a previous release <code>helm rollback</code> is possible (which helps narrow down issue being specific to current release) 
- For failed pods, investigate root cause and discuss with teams as required (e.g. pods not starting due to missing keyvault secrets)
- For pods where images beginning with <em>pr-</em> aren&rsquo;t found (has been seen quite a few times on previous cluster rebuilds) this is often due to the image no longer existing in the ACR. In these instances you will need to either reach out to the app teams to get it updated or find the latest prod image for that pod in the ACR and put in a PR to fix like this one done previously - https://github.com/hmcts/cnp-flux-config/pull/17115/files.</p>
<h4 id="before-switchover-of-a-cluster">Before switchover of a cluster</h4>
<p>Demo runs only one cluster at a time due to some limitations in the current setup.</p>

<ul>
<li>Build the other demo cluster:

<ul>
<li>CFT: Use the <a href="https://dev.azure.com/hmcts/CNP/_build?definitionId=677&amp;_a=summary">pipeline</a> in apply mode to build either 00/01 demo cluster.</li>
<li>SDS: Uncomment 00/01 cluster in this file <a href="https://github.com/hmcts/aks-sds-deploy/blob/master/environments/aks/demo.tfvars">this file</a> in a PR and merge into master, then run the <a href="https://dev.azure.com/hmcts/PlatformOperations/_build?definitionId=482">pipeline</a> in apply mode.</li>
</ul></li>
<li>Check whether all deployments/ apps are up. <code>kubectl get hr -A</code> gives a quick snapshot of progress.</li>
<li>Check that Load Balancer IPs have been assigned <code>kubectl get svc -n admin</code>, one external-ip for each instance of traefik.</li>
<li>Check all pods are deployed and running. Compare with pods status reference taken pre-deployment.</li>
</ul>
<h4 id="after-deployment-of-cluster-and-all-above-health-conditions-met">After Deployment of Cluster, and all above health conditions met</h4>

<ul>
<li>Swap backend application gateway in <a href="https://github.com/hmcts/azure-platform-terraform/pull/622/files">azure-platform-terraform</a>, or <a href="https://github.com/hmcts/sds-azure-platform/blob/master/environments/demo/demo.tfvars#L7">sds-azure-platform</a> &ndash; IP of traefik-private service.</li>
<li>Swap active external dns deployments to route traffic to new cluster <a href="https://github.com/hmcts/cnp-flux-config/pull/14659/files">example CFT PR</a>, <a href="https://github.com/hmcts/sds-flux-config/pull/2356/files">example SDS PR</a></li>
<li>Delete all ingress on the old cluster to ensure external-dns deletes it&rsquo;s existing records and the new cluster can repopulate DNS:</li>
</ul>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>kubectl delete ingress --all-namespaces -l app.kubernetes.io/managed-by=Helm
</code></pre></div>
<ul>
<li>Delete old cluster:

<ul>
<li>CFT: We are currently manually stopping and deleting the old cluster in the Azure Portal for CFT until we have a code solution similar to SDS.</li>
<li>SDS: Remove old cluster code from <a href="https://github.com/hmcts/aks-sds-deploy/blob/master/environments/aks/demo.tfvars">this file</a>, like in this <a href="https://github.com/hmcts/aks-sds-deploy/pull/208">PR</a>, once merged into master run the <a href="https://dev.azure.com/hmcts/PlatformOperations/_build?definitionId=482">pipeline</a> in apply mode.
## Known issues</li>
</ul></li>
</ul>
<h3 id="neuvector">Neuvector</h3>

<ol>
<li><code>admission webhook &quot;neuvector-validating-admission-webhook.neuvector.svc&quot; denied the request:</code>, these alerts can be seen on <code>aks-neuvector-&lt;env&gt;</code> slack channels

<ul>
<li>This happens when neuvector is broken. </li>
<li>Check events and status of neuvector helm release. </li>
<li>Delete Neuvector Helm release to see if it comes back fine.</li>
</ul></li>
<li>Neuvector fails to install.

<ul>
<li>Check if all enforcers come up in time, they could fail to come if nodes are full.</li>
<li>If they keep failing with race conditions, it could be due to backups being corrupt.</li>
<li>Usually <code>policy.backup</code> and <code>admission_control.backup</code> are the ones you need to delete from Azure file share if they are corrupt.</li>
</ul></li>
</ol>
