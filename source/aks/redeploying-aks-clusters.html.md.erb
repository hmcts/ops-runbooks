---
title: Updating AKS Clusters
weight: 30
---

# <%= current_page.data.title %>

This run book documents some tasks that we have to perform when updating any of the AKS Clusters

## Cluster Order of Updating

This is the normal order we use when updating clusters, it does not have to exactly follow it:

- Sbox             - (cft-sbox-00-aks, cft-sbox-01-aks/ ss-sbox-00-aks, ss-sbox-01-aks)
- Ptlsbox          - (cft-ptlsbox-00-aks/ ss-ptlsbox-00-aks)
- ITHC             - (cft-ithc-00-aks, cft-ithc-01-aks/ ss-ithc-00-aks, ss-ithc-01-aks)
- Preview          - (cft-preview-01-aks/ ss-dev-01-aks)
- Demo             - (cft-demo-00-aks, cft-demo-01-aks/ ss-demo-00-aks, ss-demo-01-aks)
- Perftest         - (cft-perftest-00-aks, cft-perftest-01-aks/ ss-test-00-aks, ss-test-01-aks)
- AAT              - (cft-aat-00-aks, cft-aat-01-aks/ ss-stg-00-aks, ss-stg-01-aks)
- Production       - (prod-00-aks, prod-01-aks/ ss-prod-00-aks, ss-prod-01-aks) 
- Ptl              - (cft-ptl-00-aks/ ss-ptl-00-aks) 

## Pods and HRs Health Checks

Put a message out into the #cloud-native-announce channel to notify people which cluster will be upgraded (do not use @here as this is a notification not an announcement).

Before upgrading any of the clusters run the following query on the cluster you are planning to upgrade to find out the statuses of the pods and hrs:

```
kubectl get pods -A | wc -l > pods_total_count && kubectl get pods -A > pods_all_status && kubectl get pods -A | awk '!/(Running|Succeeded)/' > pods_not_running_or_succeeded && kubectl get hr -A > hr_all_status
```

Three files will be saved in the directory from which you are running the above command. 

- Investigate failed helm releases and missing pods as required

- For failed helm releases where pods are not deployed, test if rolling back to a previous release `helm rollback` is possible (which helps narrow down issue being specific to current release) 

- For failed pods, investigate root cause and discuss with teams as required (e.g. pods not starting due to missing keyvault secrets)

- For pods where images beginning with *pr-* are not found (has been seen quite a few times on previous cluster rebuilds) this is often due to the image no longer existing in the ACR. In these instances you will need to either reach out to the app teams to get it updated or find the latest prod image for that pod in the ACR and put in a PR to fix like this [one](https://github.com/hmcts/cnp-flux-config/pull/17115/files) done previously.

- For a non production environment (or a non live application in production) if the team cannot fix the issue quickly and it is not a common component then comment out the application in flux for the environment to unblock the upgrade

## Common Upgrade Steps

The upgrade process begins by removing the cluster you are going to upgrade from the application gateway. 

  - [PR example for SDS here](https://github.com/hmcts/sds-azure-platform/pull/473/files)
  - [PR example for CFT here](https://github.com/hmcts/azure-platform-terraform/pull/580/files)

Unsure which IP belongs to which cluster? Check the front end IP of the kubernetes-internal loadbalancer: 

[portal.azure.com](https://portal.azure.com) > Load Balancer > search for 'kubernetes-internal' > filter by the Resource Group > click on the load balancer returned by the search > click on Frontend IP Configuration > the IP Address is the one you need

##  Environment Specific Steps
Some environments do have some additional requirements / checks that need to be confirmed prior to upgrading any of its clusters.

### Ptlsbox, Ptl

Make an announcement that Jenkins will be briefly unavailable in the cloud-native-announce slack channel. 
This environment is best to be done early in the morning.
See [Jenkins instances](/jenkins/) to check they are running after upgrades.

### Dev/Preview, AAT/Staging
- Change Jenkins to use the other cluster that is not going to be redeployed
  - [SDS example](https://github.com/hmcts/sds-flux-config/pull/2637)
  - [CFT example](https://github.com/hmcts/cnp-flux-config/pull/7138)
- If the configuration doesn't update after your pull request has been merged, you can change it manually in the [Jenkins Configuration](https://build.platform.hmcts.net/configure)

## Upgrading the Cluster

Find the cluster you want to upgrade into the portal.

Cluster Configuration > Upgrade Version > in the Kubernetes version drop down menu choose the latest GA version > Click Save

This will trigger the upgrade of the cluster, which will take some time. Once this has finished you need to create a pull request to update the code to match.

- [SDS repo](https://github.com/hmcts/aks-sds-deploy/blob/master/environments/aks/ptlsbox.tfvars#L3)
- [CFT repo](https://github.com/hmcts/aks-cft-deploy/blob/main/environments/aks/demo.tfvars#L3)

We do this upgrade manually in the portal because it means we don't have terraform state issues when the upgrade times out if there was a failure due to failing pods or some other reason.

<%= warning_text('The <code>kubernetes_version</code> value only takes major version values, i.e 1.25, 1.26 and so on.') %>

Once the changes have been reviewed, approved and merged, run the pipeline for the environment you are updating to have terraform update the state:

- [SDS Pipeline](https://dev.azure.com/hmcts/PlatformOperations/_build?definitionId=482)
- [CFT Pipeline](https://dev.azure.com/hmcts/PlatformOperations/_build?definitionId=766)

Once completed compare the `hrs` and pods statuses with the ones you saved before the upgrade took place.

## After upgrading of a cluster
- Add the cluster back into the application gateway once you have confirmed the upgrade has been successful.
  - [SDS example](https://github.com/hmcts/sds-azure-platform/pull/474/files)
  - [CFT example](https://github.com/hmcts/azure-platform-terraform/pull/595/files)

## Re-building a cluster

<%= warning_text('Re-building a cluster is only done when absolutely necessary.') %>

When re-building a cluster, delete the cluster manually in portal. You will have to remove the lock on resource group if applied.

To re-build the cluster you need to run the pipeline:

  - [CFT Pipeline](https://dev.azure.com/hmcts/PlatformOperations/_build?definitionId=766)
  - [SDS Pipeline](https://dev.azure.com/hmcts/PlatformOperations/_build?definitionId=482)

Click on **Run pipeline** (blue button) in top right of Azure DevOps. Ensure that Action is set to Apply, Cluster is set to whichever one you want to build and that the environment is selected from the drop down menu. 

Click on **Run** to start the pipeline.

##  Environment Specific Re-builds
### Preview

We have the ability to create another preview cluster on demand. We do not run with two at a time as they are 75 node clusters and that would be quite expensive. Announce preview cluster swap a day in advance in the #cloud-native-announce channel

#### When the build has finished

- Check the system helm releases and pods are up in the namespaces: flux-system, kube-system, admin
- Check an external IP has been assigned: `kubectl get svc traefik -n admin`

#### How to test a specific Preview cluster without swapping over

Create a branch of the Jenkins library that forces Jenkins to use the cluster that you want to use.
See this [commit](https://github.com/hmcts/cnp-jenkins-library/commit/a64cfe6919da522780e5cfb1cb4804dd6fa3675a) as an example.
Do not use an existing branch as it may be missing changes that are required for the preview cluster to work.

Change the plum Jenkinsfile to use the Jenkins library branch you created, [example](https://github.com/hmcts/cnp-plum-recipes-service/pull/379/files), do not merge it in, but instead check in Jenkins that the pipeline run has completed without any issues.

#### After deployment of a cluster

- Change Jenkins to use the other cluster, e.g. [cnp-flux-config#19886](https://github.com/hmcts/cnp-flux-config/pull/19886/files) .
  - If not updated, you can change manually providing PR has been approved and merged [View Jenkins Configuration](https://build.platform.hmcts.net/configure)
- Swap external DNS active cluster, e.g. [cnp-flux-config#19889](https://github.com/hmcts/cnp-flux-config/pull/19889) 
- Create a test PR, normally we just make a README change to [rpe-pdf-service](https://github.com/hmcts/rpe-pdf-service) . [example PR](https://github.com/hmcts/rpe-pdf-service/pull/318)
  - Check this PR in Jenkins has successfully ran through stage [AKS deploy - preview](https://build.platform.hmcts.net/job/HMCTS_Platform/job/rpe-pdf-service/view/change-requests/job/PR-318/)
- Update the charts for jobs and steps pointing to the new preview [cnp-azuredevops-pipelines#127](https://github.com/hmcts/cnp-azuredevops-libraries/pull/127/files)
- Send across the comms on #cloud-native-announce channel regarding the swap over to the new preview cluster, see below example annoucenment:-

Hi all, Preview cluster has been swapped cft-preview-00-aks.
Login using:

```command
az aks get-credentials --resource-group cft-preview-00-rg --name cft-preview-00-aks --subscription DCD-CFTAPPS-DEV --overwrite
```

* Delete all ingress on the old cluster to ensure external-dns deletes its existing records:

```command
kubectl delete ingress --all-namespaces -l '!helm.toolkit.fluxcd.io/name,app.kubernetes.io/managed-by=Helm'
```

* Delete any orphan A records that external-dns might have missed:

_Replace X.X.X.X with the loadbalancer IP (kubernetes-internal) of the cluster you want to cleanup_

Private DNS

```command
az network private-dns record-set a list --zone-name service.core-compute-preview.internal -g core-infra-intsvc-rg --subscription DTS-CFTPTL-INTSVC --query "[?aRecords[0].ipv4Address=='X.X.X.X'].[name]" -o tsv | xargs -I {} -n 1 -P 8 az network private-dns record-set a delete --zone-name service.core-compute-preview.internal -g core-infra-intsvc-rg --subscription DTS-CFTPTL-INTSVC --yes --name {}
az network private-dns record-set a list --zone-name preview.platform.hmcts.net -g core-infra-intsvc-rg --subscription DTS-CFTPTL-INTSVC --query "[?aRecords[0].ipv4Address=='X.X.X.X'].[name]" -o tsv | xargs -I {} -n 1 -P 8 az network private-dns record-set a delete --zone-name preview.platform.hmcts.net -g core-infra-intsvc-rg --subscription DTS-CFTPTL-INTSVC --yes --name {}
```

Deletes any txt records pointing to inactive:

```bash
# Private DNS
az network private-dns record-set txt list --zone-name preview.platform.hmcts.net -g core-infra-intsvc-rg --subscription DTS-CFTPTL-INTSVC --query "[?contains(txtRecords[0].value[0], 'inactive')].[name]" -o tsv | xargs -I {} -n 1 -P 8 az network private-dns record-set txt delete --zone-name preview.platform.hmcts.net -g core-infra-intsvc-rg --subscription DTS-CFTPTL-INTSVC --yes --name {}
# Public DNS
az network dns record-set txt list --zone-name preview.platform.hmcts.net -g reformmgmtrg --subscription Reform-CFT-Mgmt --query "[?contains(txtRecords[0].value[0], 'inactive')].[name]" -o tsv | xargs -I {} -n 1 -P 8 az network dns record-set txt delete --zone-name preview.platform.hmcts.net -g reformmgmtrg --subscription Reform-CFT-Mgmt --yes --name {}
```

Once swap over is fully complete then delete the old cluster:

* Comment out the inactive preview cluster. [Example PR](https://github.com/hmcts/aks-cft-deploy/pull/485). Once PR has been approved and merged run [this pipeline](https://dev.azure.com/hmcts/PlatformOperations/_build?definitionId=766)

### AAT

#### After deployment of a cluster
- Update Civil DNS entries to point to the active Jenkins cluster (until they switch to using platform-hmcts-net). [Update active jenkins-cluster IP value in DNS](https://github.com/hmcts/azure-private-dns/pull/428/files) and [update existing platform-hmcts-net entries](https://github.com/hmcts/azure-private-dns/pull/430/files).

### Production

#### Before deployment of a cluster

Every Production change which involves taking down a cluster is supposed to include scaling up of the apps (these can change). CFT ONLY:

- idam-api (scale from 4 to 8)
- ccd-data-store-api (scale from 15 to 30)
- ccd-definition-store-api (scale from 4 to 8)
- dm-store (scale from 7 to 14)

*(idam-api requires 8 pods to be split up across 2 clusters and handle the load.  If we are bringing a cluster down then we need to ensure the other cluster has 8 pods for idam-api)*

This scaling is done via a [PR](https://github.com/hmcts/cnp-flux-config/pull/7245)

Scaling to happen just before a cluster has been removed from AGW. 

- Create PR to scale apps and merge. 
- Check cluster that will not be removed to confirm pods have scaled
- Merge PR to remove a cluster from AGW

#### After deployment of a cluster
- Add the cluster back into AGW once you have confirmed deployment has been successful. [PR example here](https://github.com/hmcts/azure-platform-terraform/pull/595).
- Revert merge for scaling pods & merge [PR example here](https://github.com/hmcts/cnp-flux-config/pull/7245)
- Confirm pods are back to correct numbers after revert

### Perftest

#### Before deployment of a cluster

- Confirm that the environment is not being used with Nickin Sitaram before starting. 
- Use slack channel pertest-cluster for communication.
- Scale the number of active nodes, increase by 5 nodes if removing a custer. 
  Reason for this CCD and IDAM will auto-scale the number of running pods when a cluster is taken out of service for a upgrade.

#### After deployment of a cluster
- Check all pods are deployed and running. Compare with pods status reference taken pre-deployment

## Known issues

### Neuvector

1. `admission webhook "neuvector-validating-admission-webhook.neuvector.svc" denied the request:`, these alerts can be seen on `aks-neuvector-<env>` slack channels
   - This happens when neuvector is broken. 
   - Check events and status of neuvector helm release. 
   - Delete Neuvector Helm release to see if it comes back fine.
2. Neuvector fails to install.
   - Check if all enforcers come up in time, they could fail to come if nodes are full.
   - If they keep failing with race conditions, it could be due to backups being corrupt.
   - Usually `policy.backup` and `admission_control.backup` are the ones you need to delete from Azure file share if they are corrupt.
