---
title: Updating AKS Clusters
weight: 30
---

# <%= current_page.data.title %>

This run book documents some tasks that we have to perform when updating any of the AKS Clusters

## Cluster Order of Updating

This is the normal order we use when updating clusters, it doesn't have to exactly follow it:

- Sbox             - (cft-sbox-00-aks, cft-sbox-01-aks/ ss-sbox-00-aks, ss-sbox-01-aks)
- Ptlsbox          - (cft-ptlsbox-00-aks/ ss-ptlsbox-00-aks)
- ITHC             - (cft-ithc-00-aks, cft-ithc-01-aks/ ss-ithc-00-aks, ss-ithc-01-aks)
- Preview          - (cft-preview-01-aks/ ss-dev-01-aks)
- Demo             - (cft-demo-00-aks, cft-demo-01-aks/ ss-demo-00-aks, ss-demo-01-aks)
- Perftest         - (cft-perftest-00-aks, cft-perftest-01-aks/ ss-test-00-aks, ss-test-01-aks)
- AAT              - (cft-aat-00-aks, cft-aat-01-aks/ ss-stg-00-aks, ss-stg-01-aks)
- Production       - (prod-00-aks, prod-01-aks/ ss-prod-00-aks, ss-prod-01-aks) 
- Ptl              - (cft-ptl-00-aks/ ss-ptl-00-aks) 

## Pods and HRs Health Checks

Put a message out into the #cloud-native-announce channel to notify people which cluster will be upgraded (do not use @here as this is a notification not an announcement).

Before upgrading any of the clusters run the following query on the cluster you are planning to upgrade to find out the statuses of the pods and hrs:

```
kubectl get pods -A | wc -l > pods_total_count && kubectl get pods -A > pods_all_status && kubectl get pods -A | awk '!/(Running|Succeeded)/' > pods_not_running_or_succeeded && kubectl get hr -A > hr_all_status
```

Three files will be saved in the directory from which you are running the above command. 

- Investigate failed helm releases and missing pods as required

- For failed helm releases where pods are not deployed, test if rolling back to a previous release `helm rollback` is possible (which helps narrow down issue being specific to current release) 

- For failed pods, investigate root cause and discuss with teams as required (e.g. pods not starting due to missing keyvault secrets)

- For pods where images beginning with *pr-* are not found (has been seen quite a few times on previous cluster rebuilds) this is often due to the image no longer existing in the ACR. In these instances you will need to either reach out to the app teams to get it updated or find the latest prod image for that pod in the ACR and put in a PR to fix like this [one](https://github.com/hmcts/cnp-flux-config/pull/17115/files) done previously.

- For a non production environment (or a non live application in production) if the team cannot fix the issue quickly and it is not a common component then comment out the application in flux for the environment to unblock the upgrade

##  Environment specifics
Some environments do have some additional requirements/checks that need to be confirmed prior to upgrading any of its clusters.

### Sandbox
#### Before upgrading of a cluster
- Remove the cluster you are going to upgrade from the AGW. [PR example for SDS here](https://github.com/hmcts/sds-azure-platform/pull/473/files) / [PR example for CFT here](https://github.com/hmcts/azure-platform-terraform/pull/580/files)
- Unsure which IP belongs to which cluster? Check the front end IP of the kubernetes-internal loadbalancer

#### After upgrading of a cluster
- Add the cluster back into AGW once you have confirmed the upgrade has been successful. [PR example for SDS here](https://github.com/hmcts/sds-azure-platform/pull/474/files) / [PR example for CFT here](https://github.com/hmcts/azure-platform-terraform/pull/595/files)

### Ptlsbox, Ptl

Make an announcement that Jenkins will be briefly unavailable in the cloud-native-announce slack channel. 
This environment is best to be done early in the morning.
See [Jenkins instances](/jenkins/) to check they are running after upgrades.

### Demo
#### Before upgrading of a cluster
- Remove the cluster you are going to upgrade from the AGW. [PR example for SDS here](https://github.com/hmcts/sds-azure-platform/pull/473/files) / [PR example for CFT here](https://github.com/hmcts/azure-platform-terraform/pull/580/files)
- Unsure which IP belongs to which cluster? Check the front end IP of the kubernetes-internal loadbalancer

#### After upgrading of a cluster
- Add the cluster back into AGW once you have confirmed the upgrade has been successful. [PR example for SDS here](https://github.com/hmcts/sds-azure-platform/pull/474/files) / [PR example for CFT here](https://github.com/hmcts/azure-platform-terraform/pull/595/files)

### ITHC, DEV, Perftest, AAT
#### Before upgrading of a cluster
- Remove the cluster you are going to upgrade from the AGW. [PR example for SDS here](https://github.com/hmcts/sds-azure-platform/pull/473/files) / [PR example for CFT here](https://github.com/hmcts/azure-platform-terraform/pull/580/files)
- Unsure which IP belongs to which cluster? Check the front end IP of the kubernetes-internal loadbalancer
- Change Jenkins to use the other cluster that is not going to be redeployed, e.g. [PR example for SDS here](https://github.com/hmcts/sds-flux-config/pull/2637/files) / [PR example for CFT here](https://github.com/hmcts/cnp-flux-config/pull/7138/files)
  - If not updated, you can change manually providing PR has been approved and merged [View Jenkins Configuration](https://build.platform.hmcts.net/configure)

#### After upgrading of a cluster
- Add the cluster back into AGW once you have confirmed the upgrade has been successful. [PR example for SDS here](https://github.com/hmcts/sds-azure-platform/pull/474/files)/ [PR example for CFT here](https://github.com/hmcts/azure-platform-terraform/pull/595/files)

## Upgrading the Cluster

Find the cluster you want to upgrade into the portal > Cluster Configuration > Upgrade Version > in the Kubernetes version drop down menu choose the latest GA version > Click Save

This will trigger the upgrade of the cluster, which will take some time. Once this has finished you need to create a PR in the [SDS repo](https://github.com/hmcts/aks-sds-deploy/blob/master/environments/aks/ptlsbox.tfvars#L3) / [CFT repo](https://github.com/hmcts/aks-cft-deploy/blob/main/environments/aks/demo.tfvars#L3) changing the kubernetes_version value
  * Note: The kubernetes_version value only takes major version values, i.e 1.25, 1.26 and so on.

Once the changes have been reviewed, approved and merged, run the [SDS Pipeline](https://dev.azure.com/hmcts/PlatformOperations/_build?definitionId=482) / [CFT Pipeline](https://dev.azure.com/hmcts/PlatformOperations/_build?definitionId=766). Once completed compare the hrs and pods statuses with the ones you saved before the upgrade took place.

#### Issues Upgrading the Cluster
If a non-prod cluster upgrade fails due to a pod, which is in a Crashed state, and the team are not using the application, it can be disabled. [Example PR](https://github.com/hmcts/sds-flux-config/pull/3168/files). Once disabled and the cluster successfully upgrades and is back up and running, you can re-enable the application.

## Re-deploying a cluster
### Re-building a cluster is only done when absolutely necessary. If an environment is not listed below, the process of preparing the cluster to be re-build is the same as the upgrade steps, with the exception that you will need to delete the cluster from the portal if a rebuild is required
### Ptlsbox

#### Before deployment of a cluster

As this environment only tends to be used by IDAM, you need to confirm with Paul Verity that the Sandbox Jenkins instance which sits on this cluster is not being used and that he is ok with the work to go ahead. The Sandbox Jenkins instance can be logged into [CFT Jenkins](https://sandbox-build.platform.hmcts.net/) / [SDS Jenkins](https://sds-sandbox-build.platform.hmcts.net/securityRealm/finishLogin).

Confirm that the service job within Jenkins runs successfully [cnp-plum-recipes-service](https://sandbox-build.platform.hmcts.net/job/HMCTS_Sandbox_CNP/job/cnp-plum-recipes-service/job/master/) /[sds-toffee-recipe-service](https://sds-sandbox-build.platform.hmcts.net/job/HMCTS_Sandbox/job/sds-toffee-recipes-service/).

* Stop the cluster and once the cluster has fully stopped take a snapshot of the disk referenced [CFT](https://github.com/hmcts/cnp-flux-config/blob/85d61449e8633c6a975798c01e7ce155c9861c7e/apps/jenkins/jenkins/sbox-intsvc/disk.yaml#L8) / [SDS](https://github.com/hmcts/sds-flux-config/blob/master/apps/jenkins/jenkins/ptlsbox/disk.yaml#L9). 

* After cluster has re-deployed, refer to the snapshot previously taken and create a new disk from it named as **jenkins-disk** ensuring it is placed in the location referenced referenced above.

#### After deployment of a cluster

Once the cluster has been redeployed and Jenkins is back up and running, test the service job again to confirm it runs successfully.

### Preview

We have the ability to create another preview cluster on demand. We don't run with two at a time as they're 75 node clusters and that would be quite expensive.

#### When the build has finished

- Check the system helm releases and pods are up: flux, flux-helm-operator, tunnelfront, coredns, nodelocaldns, - aad-pod-identity, traefik
- Check an external IP has been assigned: kubectl get service -n admin |awk '$4 ~ /^[0-9]/'
- Check OSBA is running, kubectl get pods -n osba, kubectl get pods -n catalog

#### How to test a specific Preview cluster without swapping over
[cnp-plum-recipe-service](https://github.com/hmcts/cnp-plum-recipes-service/pull/379/files)

[cnp-jenkins-library](https://github.com/hmcts/cnp-jenkins-library/compare/preview01?expand=1)

This will simulate a repo/setup is using the Preview cluster that has not been swapped, useful to test if required.
Same PR might not be re-usable when the cluster is swapped to active as DNS record could be invalid. You can verify and manually tweak the records to be able to reuse it.
#### After deployment of a cluster

- Change Jenkins to use the other cluster, e.g. [cnp-flux-config#19886](https://github.com/hmcts/cnp-flux-config/pull/19886/files) .
  - If not updated, you can change manually providing PR has been approved and merged [View Jenkins Configuration](https://build.platform.hmcts.net/configure)
- Swap external DNS active cluster, e.g. [cnp-flux-config#19889](https://github.com/hmcts/cnp-flux-config/pull/19889) 
- Create a test PR, normally we just make a README change to [rpe-pdf-service](https://github.com/hmcts/rpe-pdf-service) . [example PR](https://github.com/hmcts/rpe-pdf-service/pull/318)
  - Check this PR in Jenkins has successfully ran through stage [AKS deploy - preview](https://build.platform.hmcts.net/job/HMCTS_Platform/job/rpe-pdf-service/view/change-requests/job/PR-318/)
- Update the charts for jobs and steps pointing to the new preview [cnp-azuredevops-pipelines#127](https://github.com/hmcts/cnp-azuredevops-libraries/pull/127/files)
- Send across the comms on #cloud-native-announce channel regarding the swap over to the new preview cluster, see below example annoucenment:-

Hi all, Preview cluster has been swapped cft-preview-00-aks.
Login using:

```command
az aks get-credentials --resource-group cft-preview-00-rg --name cft-preview-00-aks --subscription DCD-CFTAPPS-DEV --overwrite
```

* Delete all ingress on the old cluster to ensure external-dns deletes its existing records:

```command
kubectl delete ingress --all-namespaces -l '!helm.toolkit.fluxcd.io/name,app.kubernetes.io/managed-by=Helm'
```

* Delete any orphan A records that external-dns might have missed:

_Replace 10.101.159.250 with the loadbalancer IP (kubernetes-internal) of the cluster you want to cleanup_

Private DNS

```command
az network private-dns record-set a list --zone-name service.core-compute-preview.internal -g core-infra-intsvc-rg --subscription DTS-CFTPTL-INTSVC --query "[?aRecords[0].ipv4Address=='10.101.159.250'].[name]" -o tsv | xargs -I {} -n 1 -P 8 az network private-dns record-set a delete --zone-name service.core-compute-preview.internal -g core-infra-intsvc-rg --subscription DTS-CFTPTL-INTSVC --yes --name {}
az network private-dns record-set a list --zone-name preview.platform.hmcts.net -g core-infra-intsvc-rg --subscription DTS-CFTPTL-INTSVC --query "[?aRecords[0].ipv4Address=='10.101.159.250'].[name]" -o tsv | xargs -I {} -n 1 -P 8 az network private-dns record-set a delete --zone-name preview.platform.hmcts.net -g core-infra-intsvc-rg --subscription DTS-CFTPTL-INTSVC --yes --name {}
```

Public DNS

```command
az network dns record-set a list --zone-name demo.platform.hmcts.net -g reformmgmtrg --subscription Reform-CFT-Mgmt --query "[?aRecords[0].ipv4Address=='20.90.254.226'].[name]" -o tsv | xargs -I {} -n 1 -P 8 az network dns record-set a delete --zone-name demo.platform.hmcts.net -g reformmgmtrg --subscription Reform-CFT-Mgmt --yes --name {}
```

Deletes any txt records pointing to inactive:

```bash
# Private DNS
az network private-dns record-set txt list --zone-name demo.platform.hmcts.net -g core-infra-intsvc-rg --subscription DTS-CFTPTL-INTSVC --query "[?contains(txtRecords[0].value[0], 'inactive')].[name]" -o tsv | xargs -I {} -n 1 -P 8 az network private-dns record-set txt delete --zone-name demo.platform.hmcts.net -g core-infra-intsvc-rg --subscription DTS-CFTPTL-INTSVC --yes --name {}
# Public DNS
az network dns record-set txt list --zone-name demo.platform.hmcts.net -g reformmgmtrg --subscription Reform-CFT-Mgmt --query "[?contains(txtRecords[0].value[0], 'inactive')].[name]" -o tsv | xargs -I {} -n 1 -P 8 az network dns record-set txt delete --zone-name demo.platform.hmcts.net -g reformmgmtrg --subscription Reform-CFT-Mgmt --yes --name {}
```

Once swap over is fully complete then delete the older cluster,

* Run the [Pipeline](https://dev.azure.com/hmcts/PlatformOperations/_build?definitionId=766) ensuring Action is set to Destroy, Cluster is set to cluster you plan to destroy and Environment is set to that you intend to run against before clicking on **Run**. 

### AAT

#### Before deployment of a cluster
- Remove the cluster you are going to redeploying from the AGW. [PR example for SDS here](https://github.com/hmcts/sds-azure-platform/pull/473/files) / [PR example for CFT here](https://github.com/hmcts/azure-platform-terraform/pull/580/files)
- Unsure which IP belongs to which cluster? Check the front end IP of the kubernetes-internal loadbalancer
- Change Jenkins to use the other cluster that is not going to be redeployed, e.g. [PR example for SDS here](https://github.com/hmcts/sds-flux-config/pull/2637/files) / [PR example for CFT here](https://github.com/hmcts/cnp-flux-config/pull/7138/files)
  - If not updated, you can change manually providing PR has been approved and merged [View Jenkins Configuration](https://build.platform.hmcts.net/configure)

#### After deployment of a cluster
- Add the cluster back into AGW once you have confirmed deployment has been successful. [PR example for SDS here](https://github.com/hmcts/sds-azure-platform/pull/474/files)/ [PR example for CFT here](https://github.com/hmcts/azure-platform-terraform/pull/595/files)
- Update Civil DNS entries to point to the active Jenkins cluster (until they switch to using platform-hmcts-net). [Update active jenkins-cluster IP value in DNS](https://github.com/hmcts/azure-private-dns/pull/428/files) and [update existing platform-hmcts-net entries](https://github.com/hmcts/azure-private-dns/pull/430/files).

### Production

#### Before deployment of a cluster
- Remove the cluster you are going to be redeploying from the AGW. [PR example for CFT here](https://github.com/hmcts/azure-platform-terraform/pull/594) / [PR example for SDS here](https://github.com/hmcts/sds-azure-platform/pull/473/files)
- Unsure which IP belongs to which cluster? Check the front end IP of the kubernetes-internal loadbalancer

Every Production change which involves taking down a cluster is supposed to include scaling up of the apps (these can change): CFT ONLY:

- idam-api (scale from 4 to 8)
- ccd-data-store-api (scale from 15 to 30)
- ccd-definition-store-api (scale from 4 to 8)
- dm-store (scale from 7 to 14)

*(idam-api requires 8 pods to be split up across 2 clusters and handle the load.  If we are bringing a cluster down then we need to ensure the other cluster has 8 pods for idam-api)*

This scaling is done via a [PR](https://github.com/hmcts/cnp-flux-config/pull/7245)

Scaling to happen just before a cluster has been removed from AGW. 

- Create PR to scale apps and merge. 
- Check cluster that will not be removed to confirm pods have scaled
- Merge PR to remove a cluster from AGW

#### After deployment of a cluster
- Add the cluster back into AGW once you have confirmed deployment has been successful. [PR example here](https://github.com/hmcts/azure-platform-terraform/pull/595) / [PR example for CFT here](https://github.com/hmcts/azure-platform-terraform/pull/595/files)
- Revert merge for scaling pods & merge [PR example here](https://github.com/hmcts/cnp-flux-config/pull/7245)
- Confirm pods are back to correct numbers after revert

### Perftest

#### Before deployment of a cluster

- Confirm that the environment is not being used with Nickin Sitaram before starting. 
- Use slack channel pertest-cluster for communication.
- Scale the number of active nodes, increase by 5 nodes if removing a custer. 
  Reason for this CCD and IDAM will auto-scale the number of running pods when a cluster is taken out of service for a upgrade.

#### After deployment of a cluster
- Check all pods are deployed and running. Compare with pods status reference taken pre-deployment

## To re-create a cluster:

When redeploying, delete the cluster manually in portal. You will have to remove the lock on resource group if applied.

Go to the [CFT Pipeline](https://dev.azure.com/hmcts/PlatformOperations/_build?definitionId=766)/ [SDS Pipeline](https://dev.azure.com/hmcts/PlatformOperations/_build?definitionId=482). 

Click on **Run pipeline** (blue button) in top right of Azure DevOps. Ensure that Action is set to Apply, Cluster is set to which ever one you want to build and that the environment is selected from the drop down menu. 

Click on **Run** to start the pipeline.

## Known issues

### Neuvector

1. `admission webhook "neuvector-validating-admission-webhook.neuvector.svc" denied the request:`, these alerts can be seen on `aks-neuvector-<env>` slack channels
   - This happens when neuvector is broken. 
   - Check events and status of neuvector helm release. 
   - Delete Neuvector Helm release to see if it comes back fine.
2. Neuvector fails to install.
   - Check if all enforcers come up in time, they could fail to come if nodes are full.
   - If they keep failing with race conditions, it could be due to backups being corrupt.
   - Usually `policy.backup` and `admission_control.backup` are the ones you need to delete from Azure file share if they are corrupt.
